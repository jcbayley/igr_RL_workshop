{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to reinforcement learning\n",
    "Joe Bayley - joseph.bayley@glasgow.ac.uk\n",
    "\n",
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jcbayley/igr_RL_workshop/blob/main/RL_introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is the third paradigm of machine learning, there is supervised, unsupervised and reinforcement learning. \n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/ML_summary.png\" width=\"900\"/>\n",
    "\n",
    "What makes reinforcement learning different:\n",
    " - There is no supervision, nothing to tell it what action was best, just a reward signal\n",
    " - Feedback is delayed - the information it gets about an action will only come much later after a series of actions\n",
    " - Data is not iid, what an agent does at each step in time is highly correlated to the previous steps. \n",
    " - Agent takes actions to influence its environment\n",
    "\n",
    "\n",
    "There are some other great training resources around for reinforcement learning, I'll list a few here\n",
    " - Great intro course from David Silver (AlphaGo etc) - https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n",
    " - Intro notes - Sutton and Barto - https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    " - Open AI spinning up documentation - https://spinningup.openai.com/en/latest/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key concepts/Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> Environment and Agent</h3>\n",
    "\n",
    "The environment describes the world in the agent operates.  This is usually defined as a markov process (will come back to this later), where each node is a state and the edge is an action which leads to a new state(node)\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> States and observations</h3>\n",
    "\n",
    "symbols -> $S_t, O_t$\n",
    "\n",
    "The state refers to the conditions of the environment at a given time \\\n",
    "The observations are how the agent sees the state, this could be a complete observations of the state itself, or could be only some elements of the state. \n",
    "\n",
    "---\n",
    "<h3 style=\"text-align: center\"> Actions</h3>\n",
    "\n",
    "symbols -> $a_t$\n",
    "\n",
    "The actions define the choices that an agent can make within an enironment. These actions can be discrete or continuous.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> Policy </h3>\n",
    "\n",
    "symbols -> $\\pi_\\theta(S_t)$\n",
    "\n",
    "The policy defines how an agent behaves at any time. It is approximately mapping the states(observations) to actions.  This policy can be deterministic, or can be probabalistic, this usually depends on the types of actions and type of environment. This is where a neural network could come in, mapping the current state of the environment to the best possible action.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> Reward and Return</h3>\n",
    "\n",
    "symbols -> $R_t, G$\n",
    "\n",
    "The return function defines the goal in RL. This is the information that the environment feeds back to the agent and the agent wants to maximise the total reward in the long run (also known as the return).\n",
    "\n",
    "Rewards can be stochastic values as they depend on the state of the environment.\n",
    "\n",
    "We can also define the return here which is the total return over the whole episode (1 game, 1 life etc).\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{t}^{T} R_t\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> Value function </h3>\n",
    "\n",
    "symbols -> $V(S_t)$, $Q(S_t, a_t)$\n",
    "\n",
    "Rewards give information on what is good immediately after taking an actions, whereas the value function looks at what is good over the long term. It returns the total amount of reward you could expect over the future if we started at the state $S_t$.\n",
    "\n",
    "We generally seek out actions that give high values rather than high rewards, as we know it is likely to give us a greater reward in the future.\n",
    "\n",
    "---\n",
    "\n",
    "<h3 style=\"text-align: center\"> Model </h3>\n",
    "\n",
    "The model refers to a model of the environment. This is an optional part. The model might take in the current state and an action and try to predict the next environment state and reward. These can be used for \"planning\", where the actions can be decided based on this model before it actually experiences those states. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General goal\n",
    "\n",
    "**The general goal of all this is to update your policy to maximise the reward that you get.**\n",
    "\n",
    "The loop that you would follow is:\n",
    "\n",
    " - There is and Agent in some environment that will take actions following some policy. \n",
    " - The Agent will take some action. \n",
    " - The Environment may change based on that action and return a new state and obersvation. \n",
    " - The Agent will take another action based on the observation of the state.\n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/action_environment_loop.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process\n",
    "\n",
    "Reinforcement learning problems are usually thought of as markov decision processes, where each state only depends on the previous state. \n",
    "$$\n",
    "P[S_{t+1} | S_t] = P[S_{t+1}| S_1, ... S_t]\n",
    "$$\n",
    "Almost all problems can be expressed as a markov decision process.\n",
    "\n",
    "We can make this tutorial as an mdp \n",
    "\n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/tutorial_mdp.png\" width=\"800\"/>\n",
    "\n",
    "We can also write out what reward we would gain by taking each action (this is something that would come back from the environment).\n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/tutorial_mdp_return.png\" width=\"800\"/>\n",
    "\n",
    "The return is then the total (discounted) reward from some time step t:\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_k \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "The discount factor is a way to control the return and prevent it being an infinite sum. It tells you how much you care now about what happens in the future ( do you prefer reward now or in the future).\n",
    "\n",
    "If we come back to the Value function, we can compute the value of being in some state $s_t$ as being the expected value of the return from that state,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(s) &= \\mathop{\\mathbb{E}} \\left[ G_t | S_t = s\\right] \\\\\n",
    "  &= \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...| S_t = s \\right] \\\\\n",
    "   &= \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma \\left(R_{t+2} + \\gamma R_{t+3} + ...\\right) | S_t = s \\right] \\\\\n",
    " &= \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma G_t | S_t = s \\right] \\\\\n",
    "  &= \\mathop{\\mathbb{E}} \\left[ R_{t+1} + \\gamma V(S_{t+1}) | S_t = s \\right] \\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is the Bellman equation, which says that the expected return that I will get is the immediate reward plus the discounted return that I would get from the next time step. \n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/tree_graph.png\" width=\"800\"/>\n",
    "\n",
    "Above we have just taken random actions, where each actions has some probability associated with it. However we want to make choices here, we want to choose an action that will maximise out return.\n",
    "\n",
    "To do this we need a policy, this defined the probability of taking an action in a given state.\n",
    "\n",
    "$$\n",
    "\\pi(a | s) = P\\left[A_t = a | S_t = s\\right]\n",
    "$$\n",
    "\n",
    "We can change this policy to whatever we want, with with the same goal of maximising the return we get by following that policy from now onwards.\n",
    "\n",
    "The value function above changes slightly as we will be following some policy $\\pi$, \n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathop{\\mathbb{E}}_{\\pi}\\left[G_t | S_t = s \\right]\n",
    "$$\n",
    "\n",
    "We can also write down an action-value function which depends on the action, as the different actions will give a different return. \n",
    "The Bellman equation for this is then\n",
    "\n",
    "$$\n",
    "Q_{\\pi}(s, a) =  \\mathop{\\mathbb{E}}_{\\pi} \\left[G_t | S_t = s, A_t = a \\right] \\\\\n",
    "$$\n",
    "\n",
    "For any markov decision process, there is an optimal policy $\\pi_{*}$ which is better than or equal to all other policies. All optimal policies will give the optimal value functions: \n",
    "\n",
    "$V_{\\pi_*}(s) = V_*(s)$ \\\n",
    "$Q_{\\pi_*}(s, a) = Q_*(s, a)$\n",
    "\n",
    "So what we want to do is try to find one of these optimal policies, however there is no closed form solution to the Bellman equations described about. Generally these are solved iteratively. (There is not necessarily any Neural networks introduced at this stage.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid world\n",
    "\n",
    "We can start with a simple grid world where the goal is to move to the star in the image. \n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/grid_world.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "This can be written out as a markov process, where as its a simple case we could work out each of the transition probabilities individually.\n",
    "\n",
    "We could compute the value of being in any of the positions (or state) as the distance from the goal point\n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/grid_world_value2.png\" width=\"500\"/>\n",
    "\n",
    "From this set of values we could work out the optimal policy by taking the direction which moves closer to the goal, i.e. reduces the value. The red arrows here are our optimal policy. In any given state we know what the best action to take is.\n",
    "\n",
    "<img src=\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/grid_world_policy.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Agents\n",
    "\n",
    "### Value based\n",
    " - Optimise over the value functions\n",
    " - This can be done with Dynamic programming and Monte carlo etc \n",
    " - We however, will look at Deep Q learning below\n",
    "\n",
    "### Policy based\n",
    " - Optimise the policy directly\n",
    " - Similar to above this can be done with traditional techniques\n",
    " - We will focus on policy gradients below\n",
    "\n",
    "### Actor critic\n",
    " - combines both the value function and policy to get the best of both worlds.\n",
    "\n",
    "### Model based\n",
    " - We have a model of the environment, then we plan with it by looking ahead to what happens if we follow this model. \n",
    " - We might know all the possible states, or may be able to build a model of the environment. \n",
    "\n",
    "### Model free\n",
    " - We dont learn a model of the enironment but just look at the policy and/or value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing exploitation and exploration\n",
    "\n",
    "When our agent is exploring our environment there is a balance to be struck between exploring the environment to see new states and exploiting the knowledge we have gained to follow a good policy.\n",
    "This is addressed in different ways in the methods above\n",
    "\n",
    "### Value based\n",
    " - In value based methods it is generally solved by choosing to follow the learned policy and selection a random action. i.e. There is some probability that we will take a random action. Generally this probability starts as one and decays over time. Therefore we explore more at the start and follow our policy more towards the end of training.\n",
    "\n",
    "### Policy based\n",
    " - Policy based methods have a exploration step almost built in. As we define the policy directly, we can just sample from that probability distributions, therefore we will get samples from other actions than the most probable natrually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environments\n",
    "\n",
    "We will be using gym to build a model, this is a common library which is used to build environments. Below we will show two different environments.\n",
    "1. Cartpole - this is a typical environment you will see in tutorials, its balancing a pole on the top of a cart, you can move the cart left or right.\n",
    "2. Bike ride - I  wrote a custom enviroment about a mountain bike ride so you can see the key components within each function (no promises that this is particularly well written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import a few things\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import matplotlib\n",
    "from IPython.display import HTML\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at various options within this environment like the number of available actions or what is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example there are 4 observations: The angle of the pole, the velocity of the pole, the position of the cart, the velocity of the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two actions to take, move right or move left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reset the initial position/velocty of the cart with the reset method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other important component to the environment is its ability to take an action and transition to a new state. This is done in the step function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "\n",
    "next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "print(\"Initial observation\")\n",
    "print(initial_obs[0])\n",
    "print(\"Observation after action\")\n",
    "print(next_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I thought it might be useful to write out a custom environment to show what each of the components do. We'll write it out for a simple game here, this is not essential but may be useful to see. \n",
    "In this game the bike can move up or down, it has to avoid trees, if it hits one the game is over. If it hits a jump the agents enjoyment goes back to maximum. otherwise the enjoyment decreases by one each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Environment definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these sets of classes make up the position of each of the three available objects in the game\n",
    "# they just make things a little easier to manage\n",
    "class Point(object):\n",
    "\n",
    "    def __init__(self,name, x_min,x_max,y_min,y_max):\n",
    "        self.name = name\n",
    "        self.x, self.y = 0, 0\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "\n",
    "    def set_position(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def move(self, dx, dy):\n",
    "        self.x += dx\n",
    "        self.y += dy\n",
    "\n",
    "        #self.x = np.clip(self.x, self.x_min, self.x_max)\n",
    "        self.y = np.clip(self.y, self.y_min, self.y_max-1)\n",
    "\n",
    "class Bike(Point):\n",
    "    def __init__(self, name, x_min, x_max, y_min, y_max):\n",
    "        super(Bike, self).__init__(name, x_min, x_max, y_min, y_max)\n",
    "        self.icon = 1\n",
    "\n",
    "class Tree(Point):\n",
    "    def __init__(self, name, x_min, x_max, y_min, y_max):\n",
    "        super(Tree, self).__init__(name, x_min, x_max, y_min, y_max)\n",
    "        self.icon = 2\n",
    "        \n",
    "class Jump(Point):\n",
    "    def __init__(self, name, x_min, x_max, y_min, y_max):\n",
    "        super(Jump, self).__init__(name, x_min, x_max, y_min, y_max)\n",
    "        self.icon = 3\n",
    "    \n",
    "\n",
    "class BikeRide(gym.Env):\n",
    "\n",
    "    def __init__(self,gridsize=(6,4)):\n",
    "\n",
    "        # initialise a set of variables \n",
    "        self.observation_shape = (3, gridsize[0], gridsize[1])\n",
    "        self.observation_size = np.prod(self.observation_shape[1:])\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low = np.zeros(self.observation_shape),\n",
    "            high = np.ones(self.observation_shape),\n",
    "            dtype = np.float16\n",
    "        )\n",
    "\n",
    "        # initialise the action space (up, down, dont move)\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "\n",
    "        # intitialise the canvas to zeros\n",
    "        self.canvas = np.zeros(self.observation_shape)\n",
    "\n",
    "        # set the maximum enjoyment possible\n",
    "        self.max_enjoyment = 30\n",
    "\n",
    "        # elements are the different objects on the canvas\n",
    "        self.elements = []\n",
    "\n",
    "        # set limits\n",
    "        self.y_min = 0\n",
    "        self.x_position = 1\n",
    "        self.x_min = 0\n",
    "        self.x_max = self.observation_shape[1]\n",
    "        self.y_max = self.observation_shape[2]\n",
    "\n",
    "        self.bike_position = (self.x_position,0)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the enjoyment consumed\n",
    "        self.enjoyment_left = self.max_enjoyment\n",
    "\n",
    "        # Reset the reward\n",
    "        self.ep_return  = 0\n",
    "\n",
    "        # Number of trees\n",
    "        self.tree_count = 0\n",
    "        self.jump_count = 0\n",
    "\n",
    "        # Determine a place to intialise the bike in\n",
    "        y = np.random.randint(self.y_min, self.y_max-1)\n",
    "        \n",
    "        # Intialise the bike\n",
    "        self.bike = Bike(\"bike\", self.x_min, self.x_max, self.y_min, self.y_max)\n",
    "        self.bike.set_position(self.x_position,y)\n",
    "\n",
    "        self.bike_position = (self.x_position, y)\n",
    "\n",
    "        # Intialise the elements \n",
    "        self.elements = [self.bike]\n",
    "\n",
    "        # Reset the Canvas \n",
    "        self.canvas = np.zeros(self.observation_shape) \n",
    "\n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "\n",
    "\n",
    "        # return the observation\n",
    "        return self.canvas, None\n",
    "\n",
    "    \n",
    "    def draw_elements_on_canvas(self):\n",
    "        # Init the canvas \n",
    "        self.canvas = np.zeros(self.observation_shape)\n",
    "\n",
    "        # Draw the bikes/trees/jumps on canvas\n",
    "        for elem in self.elements:\n",
    "            try:\n",
    "                self.canvas[elem.icon-1, elem.x, elem.y] = 1\n",
    "            except IndexError as e:\n",
    "                print(elem.name, e)\n",
    "                sys.exit()\n",
    "\n",
    "    def has_collided(self, elem1, elem2):\n",
    "\n",
    "        if (elem1.x == elem2.x) and (elem1.y == elem2.y):\n",
    "            return True\n",
    "        else:\n",
    "            False\n",
    "\n",
    "    def get_elements(self,):\n",
    "        return np.array([(el.x, el.y, el.icon) for el in self.elements])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Flag that marks the termination of an episode\n",
    "        done = False\n",
    "        \n",
    "        # Assert that it is a valid action \n",
    "        assert self.action_space.contains(action), f\"Invalid Action: {action}\"\n",
    "\n",
    "        # Decrease the enjoyment counter \n",
    "        self.enjoyment_left -= 1 \n",
    "        \n",
    "        # Reward for executing a step.\n",
    "        reward = 1      \n",
    "\n",
    "        # apply the action to the bike\n",
    "        if action == 0:\n",
    "            self.bike.move(0, 1)\n",
    "        elif action == 1:\n",
    "            self.bike.move(0, -1)\n",
    "        elif action == 2:\n",
    "            self.bike.move(0,0)\n",
    "\n",
    "        # Spawn a tree at the right edge with prob 0.2\n",
    "        if np.random.uniform() < 0.2:\n",
    "            \n",
    "            # Spawn a tree\n",
    "            spawned_tree = Tree(\"tree_{}\".format(self.tree_count), self.x_min, self.x_max, self.y_min, self.y_max)\n",
    "            self.tree_count += 1\n",
    "\n",
    "            # place the tree with a random height \n",
    "            tree_position = self.x_max-1, np.random.randint(self.y_min, self.y_max)\n",
    "            spawned_tree.set_position(tree_position[0], tree_position[1])\n",
    "            \n",
    "            # make sure its not placed in the same place as other elements\n",
    "            place = True\n",
    "            for elem in self.elements:\n",
    "                if not isinstance(elem, Bike):\n",
    "                    if self.has_collided(spawned_tree, elem):\n",
    "                        place = False\n",
    "            if place:\n",
    "                # Append the spawned tree to the elements currently present in Env. \n",
    "                self.elements.append(spawned_tree)    \n",
    "\n",
    "        # Spawn a jump at the right edge with prob 0.2\n",
    "        if np.random.uniform() < 0.2:\n",
    "            # Spawn a jump\n",
    "            spawned_jump = Jump(\"jump_{}\".format(self.jump_count), self.x_min, self.x_max, self.y_min, self.y_max)\n",
    "            # increase the jump count\n",
    "            self.jump_count += 1\n",
    "            # set a random initial position for the jump\n",
    "            jump_position = self.x_max-1, np.random.randint(self.y_min, self.y_max)\n",
    "            spawned_jump.set_position(jump_position[0], jump_position[1])\n",
    "            # make sure its not placed in the same place as other elements\n",
    "            place = True\n",
    "            for elem in self.elements:\n",
    "                if not isinstance(elem, Bike):\n",
    "                    if self.has_collided(spawned_jump, elem):\n",
    "                        place = False\n",
    "            if place:\n",
    "                # Append the spawned tree to the elements currently present in the Env.\n",
    "                self.elements.append(spawned_jump)   \n",
    "\n",
    "        # For elements in the Ev\n",
    "        for elem in self.elements:\n",
    "            if isinstance(elem, Tree):\n",
    "                # If the tree has reached the left edge, remove it from the Env\n",
    "                if elem.x <= self.x_min:\n",
    "                    self.elements.remove(elem)\n",
    "                else:\n",
    "                    # Move the tree left by 1 pts.\n",
    "                    elem.move(-1,0)\n",
    "                \n",
    "                # If the tree has collided.\n",
    "                if self.has_collided(self.bike, elem):\n",
    "                    # Conclude the episode and remove the bike from the Env.\n",
    "                    done = True\n",
    "                    reward = -10\n",
    "                    self.elements.remove(self.bike)\n",
    "                    break\n",
    "\n",
    "            if isinstance(elem, Jump):\n",
    "                # If the jump has reached the left edge, remove it from the Env\n",
    "                if elem.x <= self.x_min:\n",
    "                    self.elements.remove(elem)\n",
    "                else:\n",
    "                    # Move the jump left by 1 pts.\n",
    "                    elem.move(-1,0)\n",
    "                    \n",
    "                # If the jump has collided with the bike.\n",
    "                if self.has_collided(self.bike, elem):\n",
    "                    # Remove the jump from the env.\n",
    "                    \n",
    "                    # Fill the enjoyment to full\n",
    "                    self.enjoyment_left = self.max_enjoyment\n",
    "                    reward = 3\n",
    "    \n",
    "        # Increment the episodic return\n",
    "        self.ep_return += reward\n",
    "\n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "\n",
    "        # If out of enjoyment, end the episode.\n",
    "        if self.enjoyment_left == 0:\n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "        truncated = False\n",
    "\n",
    "        return self.canvas, reward, done, truncated, [ ]\n",
    "    \n",
    "    \n",
    "    def get_observation(self,):\n",
    "        return torch.from_numpy(self.canvas).to(torch.float32).unsqueeze(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent=None, random=False):\n",
    "    state,_ = env.reset()\n",
    "    d = False\n",
    "    all_positions = []\n",
    "    rewards = []\n",
    "    enjoyments = []\n",
    "    while not d:\n",
    "        # select an action based on some policy\n",
    "        if random:\n",
    "            # just a random policy, select a random action\n",
    "            action = np.random.randint(3)\n",
    "            # implement this random action in the environment\n",
    "            obs2, r, d, _,  _ = env.step(action)\n",
    "        else:\n",
    "            # select action based on some policy defined by the optimiser\n",
    "            action, _ = agent.select_action(torch.from_numpy(state).flatten().unsqueeze(0).to(torch.float32))\n",
    "            # implement this random action in the environment\n",
    "            try:\n",
    "                state, r, d, _, _ = env.step(action[0].numpy())\n",
    "            except:\n",
    "                state, r, d, _, _ = env.step(action)\n",
    "\n",
    "        # save the rewards, position of elements, and enjoyments\n",
    "        rewards.append(r)\n",
    "        elements = env.get_elements()\n",
    "        all_positions.append(elements)\n",
    "        enjoyments.append(env.enjoyment_left)\n",
    "\n",
    "    # compute the return as the sum of all the rewards\n",
    "    ep_return = np.sum(rewards)\n",
    "    \n",
    "    return all_positions, rewards, enjoyments, ep_return \n",
    "\n",
    "def plot_game_elements_old(env, elements, enjoyments):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    colors = [\"black\", \"red\", \"blue\"]\n",
    "    cmap = plt.cm.viridis\n",
    "    norm = matplotlib.colors.BoundaryNorm(np.arange(4), cmap.N)\n",
    "\n",
    "    sc = ax.scatter(elements[0][:,0], elements[0][:,1], c=elements[0][:,2]-1, cmap=cmap, s=80, norm=norm)\n",
    "    tx = ax.text(env.x_min,env.y_max,f\"Enjoyment: {enjoyments[0]}\")\n",
    "    ax.set_xlim([env.x_min-1, env.x_max+1])\n",
    "    ax.set_ylim([env.y_min-1, env.y_max+1])\n",
    "\n",
    "\n",
    "    def update(frame):\n",
    "        sc.set_offsets(elements[frame][:,:2])\n",
    "        sc.set_array(elements[frame][:,2]-1)\n",
    "        tx.set_text(f\"Enjoyment: {enjoyments[frame]}\")\n",
    "\n",
    "        return sc, \n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(elements), blit=True)\n",
    "\n",
    "    return fig, ani\n",
    "\n",
    "def plot_game_elements(env, elements, enjoyments):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    images = [\n",
    "        imageio.imread('https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/bike.png'),\n",
    "        imageio.imread(\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/tree.jpeg\"),\n",
    "        imageio.imread(\"https://media.githubusercontent.com/media/jcbayley/igr_RL_workshop/main/jump.jpeg\")\n",
    "    ]\n",
    "\n",
    "\n",
    "    #images = [plt.imread(f) for f in imageloads]\n",
    "\n",
    "    zooms = [\n",
    "        0.04,\n",
    "        0.1,\n",
    "        0.07\n",
    "    ]\n",
    "\n",
    "    annotation_boxes = []\n",
    "    bx, by, bim = None, None, None\n",
    "    for i, (x, y, im) in enumerate(elements[0]):\n",
    "        if im == 1:\n",
    "            bx, by, bim = x,y,im \n",
    "            continue\n",
    "        bike_imagebox = OffsetImage(images[im-1], zoom=zooms[im-1])\n",
    "        ab = AnnotationBbox(bike_imagebox, (x, y), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "        annotation_boxes.append(ab)\n",
    "\n",
    "    imagebox = OffsetImage(images[bim-1], zoom=zooms[bim-1])\n",
    "    ab = AnnotationBbox(imagebox, (bx, by), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "    annotation_boxes.append(ab)\n",
    "\n",
    "    tx = ax.text(env.x_min,env.y_max,f\"Enjoyment: {enjoyments[0]}\")\n",
    "    ax.set_xlim([env.x_min-1, env.x_max+1])\n",
    "    ax.set_ylim([env.y_min-1, env.y_max+1])\n",
    "    ax.tick_params(axis=\"both\", which=\"both\",bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "\n",
    "\n",
    "    def update(frame):\n",
    "\n",
    "        for ab in annotation_boxes:\n",
    "            ab.remove()\n",
    "        annotation_boxes.clear()\n",
    "        bx, by, bim = None, None, None\n",
    "        for i, (x, y, im) in enumerate(elements[frame]):\n",
    "            if im == 1:\n",
    "                bx, by, bim = x,y,im \n",
    "                continue\n",
    "            imagebox = OffsetImage(images[im-1], zoom=zooms[im-1])\n",
    "            ab = AnnotationBbox(imagebox, (x, y), frameon=False)\n",
    "            ax.add_artist(ab)\n",
    "            annotation_boxes.append(ab)\n",
    "        \n",
    "        imagebox = OffsetImage(images[bim-1], zoom=zooms[bim-1])\n",
    "        ab = AnnotationBbox(imagebox, (bx, by), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "        annotation_boxes.append(ab)\n",
    "\n",
    "        tx.set_text(f\"Enjoyment: {enjoyments[frame]}\")\n",
    "\n",
    "        return annotation_boxes\n",
    "\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(elements)-1, blit=True)\n",
    "    return fig, ani\n",
    "\n",
    "def plot_losses(returns, losses, game_lengths, window_length=20, log=True):\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(9,4))\n",
    "    ax[0].plot(np.arange(len(returns)), returns, \".\")\n",
    "    ax[0].plot(np.arange(len(returns)-window_length+1), np.convolve(returns, np.ones(window_length)/window_length, mode='valid'),)\n",
    "    ax[0].set_ylabel(\"returns\")\n",
    "    ax[1].plot(np.arange(len(losses)), losses)\n",
    "    if log:\n",
    "        ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_ylabel(\"Loss\")\n",
    "    ax[2].plot(np.arange(len(game_lengths)), game_lengths, \".\")\n",
    "    ax[2].plot(np.arange(len(game_lengths)-window_length+1), np.convolve(game_lengths, np.ones(window_length)/window_length, mode='valid'),)\n",
    "    ax[2].set_ylabel(\"Epidode duration\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BikeRide((6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_all_positions, rand_rewards, rand_enjoyments, rand_ep_return  = play_game(env, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ani = plot_game_elements(env, rand_all_positions, rand_enjoyments)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques\n",
    "\n",
    "There are many different techniques and algorithms within reinforcement learning, I will only highlight two below which appear in quite a few different places and are helpful for introducing concepts. Known as Policy gradients and Q-learning.\n",
    "\n",
    "There are methods such as value and policy iteration which optimise the value functions and policy by iteration and dynamic programming, however, these are rarely applicable to real world problems, therefore I will mostly look at Neural Network based approaches below. These are also model free approaces, as in general we may not know the entire MDP before starting the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a method to optimise the action-value function that we defined earlier, where the goal is to maximise the return we get at the end of the episode.\n",
    "\n",
    "In Q-learning we are trying to maximise the action value function, this is the expectation over the return $R$ given we started in some state $s$, took some action $s$ and followed some policy $\\pi$ for all the future.\n",
    "\n",
    "$$\n",
    "Q_\\pi(s,a)=\\mathop{\\mathbb{E}}_{\\pi}\\left[G_t \\mid S_t=s,\\ A_t=a\\right].\n",
    "$$\n",
    "\n",
    "Here we are going to model the Q value with a neural network, so it would see some state and output a probability distribution over actions.\n",
    "\n",
    "\n",
    "If we let the episode run to the end, we could store all of the estimated Q values for each state and action and all of the real returns, and update our model based on how well we predicted the future return. i.e.\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left(G_t - Q(S_t, A_t)\\right)\n",
    "$$\n",
    "\n",
    "where alpha is a learning rate.\n",
    "\n",
    "### TD learning\n",
    "\n",
    "The way we actually implement this below is using Temporal differece learning (TD-learning). This allows us to start learning before an episode is complete, we dont have to wait until the end. We can use our value function to predict how well we will do in the future and update our model based on that.\n",
    "\n",
    "We can think of this like so:\n",
    "1. We are in some state S_t, the value function tells us how much return we expect to get from this state.\n",
    "2. We take some action and end up in state S_{t+1}, this new state has a value function estimating how much return we get from this new state.\n",
    "3. We can update out original model based on how well it predicted that return. (updating a guess towards a guess.)\n",
    "\n",
    "So how can we write out the update to the value function above, now we dont know the true return as we have not completed the episode, however, we can still update it with an estimate of the return we get from the next update.\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\left(R_t + \\gamma Q(S_{t+1}, A_{t+1})  - Q(S_t, A_t)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "### Neural network updates\n",
    "\n",
    "Now if we write the Q function as a neural network it is generally parameterised by some parameters $\\theta$ ($Q_{\\pi}^{\\theta}(s, a)$), so we essentially want to update the parameters theta, where we update them in the direction of their derivative (gradient descent). where looking above the update is just the difference between the estimated value function and the \"true\" value function.\n",
    "\n",
    "$$\n",
    "Loss = \\mathop{\\mathbb{E}}_{\\pi} \\left[(Q_{\\pi}(s, a) - Q_{\\pi}^{\\theta}(s, a))^2  \\right]\n",
    "$$\n",
    "\n",
    "The True action-value function could be our actual returns $G_t$ if we let an episode finish and compute these values. This is essentially supervised learning on data we have collected.\n",
    "\n",
    "The True action-value function can also be our estimate of the action-value function from the next state. This is a bit like doing supervised learning on the (estimated) returns that we got.\n",
    "\n",
    "We can rewrite this for our estimated Q value as\n",
    "\n",
    "$$\n",
    "Loss = \\mathop{\\mathbb{E}}_{\\pi} \\left[(r_{t+1} + \\gamma \\max_a Q_{\\pi_t}(s_{t+1}, a) - Q_{\\pi}^{\\theta}(s_t, a_t))^2  \\right]\n",
    "$$\n",
    "\n",
    "Learning this action value function means we can then predict which is the best action to take by taking the maximum Q-value over all the actions in a given state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "    \n",
    "# setup a simple q network \n",
    "class QNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_observations, 128)\n",
    "        self.layer2 = torch.nn.Linear(128, 128)\n",
    "        self.layer3 = torch.nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "# Define the experience replay buffer\n",
    "# the replay buffer stores the training data for the network\n",
    "# it consists of:\n",
    "# the current state\n",
    "# the action taken from this state\n",
    "# the reward that was gained from taking that action from the state\n",
    "# the state that we transitioned to by performing that action \n",
    "# whether the state was a terminal one or not\n",
    "    \n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # this has a buffer of the about quantities of some desired length (usually restricted by memory)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "# Deep Q-Learning agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon_start=1.0, epsilon_decay=0.995, epsilon_min=0.01, buffer_size=10000, batch_size=64):\n",
    "        # environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # parameters associated with the action choice probability\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Q-networks \n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.target_q_network = QNetwork(state_size, action_size)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(self.q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Experience replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size), None\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.FloatTensor(state))\n",
    "                return int(torch.argmax(q_values)), None\n",
    "\n",
    "    def train(self):\n",
    "        # Sample a batch from experience replay\n",
    "        batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "\n",
    "        states = torch.FloatTensor(states).to(torch.float32)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).to(torch.float32)\n",
    "        next_states = torch.FloatTensor(next_states).to(torch.float32)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q-values for current states\n",
    "        q_values = self.q_network(states)\n",
    "\n",
    "        # Q-values for next states using target Q-network\n",
    "        next_q_values = self.target_q_network(next_states).detach()\n",
    "\n",
    "        # Compute target Q-values\n",
    "        # this takes the maxmimum over the \n",
    "        target_q_values = rewards + self.gamma * (1 - dones) * torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "        # Compute loss\n",
    "        # this is the difference between the target qvalues and those from one (or more) step in the past\n",
    "        loss = torch.nn.MSELoss()(q_values.gather(1, actions.view(-1, 1)), target_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.q_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target Q-network periodically\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # Update target Q-network with the weights of the current Q-network\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        # Add experience to replay buffer\n",
    "        self.replay_buffer.add_experience(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dqn = BikeRide((6, 4))\n",
    "# you can also run this on the cart pole problem by uncommenting the line below\n",
    "#env_dqn = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(np.prod(env_dqn.observation_space.shape), env_dqn.action_space.n, buffer_size=10000, learning_rate=0.5e-4)\n",
    "# the agent needs slight modification of inputs for the cartpole problem as below\n",
    "#dqn_agent = DQNAgent(env_dqn.observation_space.shape[0], env_dqn.action_space.n, buffer_size=10000, learning_rate=0.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of games(episodes) we want it to train for\n",
    "num_episodes = 2000\n",
    "printdiv = num_episodes//20 # how often to pring outputs\n",
    "update_frequency = 20 # how often do we want to update our target network with out current values\n",
    "# set some quantities to track\n",
    "dqn_returns = []\n",
    "dqn_losses = []\n",
    "dqn_game_lengths = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env_dqn.reset()\n",
    "    #popt.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    it = 0\n",
    "    while not done:\n",
    "        # select the action based on the dqn model\n",
    "        action, _ = dqn_agent.select_action(torch.from_numpy(state.flatten()).unsqueeze(0).to(torch.float32))\n",
    "        # take the action in the enviromnment and return next state and reward\n",
    "        next_state, reward, terminated, truncated, [ ] = env_dqn.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # save the actions, rewards and states to memory\n",
    "        dqn_agent.memorize(state.flatten(), action, reward, next_state.flatten(), done)\n",
    "\n",
    "        # update the state with the new one\n",
    "        state = next_state\n",
    "        \n",
    "        # have a little delay before training so some data can be saved to memory (can set this to batch size)\n",
    "        if episode > 10:\n",
    "            loss = dqn_agent.train()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        if it % update_frequency == 0:\n",
    "            dqn_agent.update_target_network()\n",
    "        \n",
    "        #Place a limit on how many moves the agent can make in training\n",
    "        if it > 1000:\n",
    "            done = True\n",
    "        # save the total reward (return)\n",
    "        total_reward += reward\n",
    "        it += 1\n",
    "    \n",
    "    dqn_game_lengths.append(it)\n",
    "    dqn_returns.append(total_reward)\n",
    "    dqn_losses.append(loss)\n",
    "\n",
    "    if episode % printdiv == 0:\n",
    "        print(f\"Episode: {episode}, reward: {reward}, return: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(dqn_returns, dqn_losses, dqn_game_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_all_positions, dqn_rewards, dqn_enjoyments, dqn_ep_return  = play_game(env_dqn, agent=dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an animate the game\n",
    "fig,ani = plot_game_elements(env_dqn,dqn_all_positions,dqn_enjoyments)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "Policy gradient methods compute the gradient of the loss\n",
    "\n",
    "We want to maximise the expectation value of the return given our policy.\n",
    "\n",
    "$$\n",
    "J(\\pi_{\\theta}) = \\mathop{\\mathbb{E}}_{\\pi_{\\theta}}(G_t)\n",
    "$$\n",
    "\n",
    "where $G_t$ here is out discounted return.\n",
    "\n",
    "The goal is to find the parameters $\\theta$ that maximise $J$. As we may be used to seeing within machine learning we can just update the parameters in the direction of the gradient of $J$\n",
    "\n",
    "$$\n",
    "    \\theta_{k+1} = \\theta_k + \\alpha\\nabla J(\\pi_{\\theta_k}) \n",
    "$$\n",
    "\n",
    "If we know the gradient of $J$ then we can compute this (summary is below but see https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html for more details)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta}) &= \\nabla_{\\theta} \\mathop{\\mathbb{E}} \\left[ G_t \\right] \\\\\n",
    "&= \\nabla_{\\theta} \\int p(\\tau | \\theta) G_t d \\tau \\\\\n",
    "&=  \\int \\nabla_{\\theta} p(\\tau | \\theta) G_t d \\tau \\\\\n",
    "&= \\int p(\\tau | \\theta) \\nabla_{\\theta} \\log p(\\tau | \\theta) G_t \\\\\n",
    "&= \\mathop{\\mathbb{E}} \\left[ \\log \\nabla_{\\theta} p(\\tau | \\theta) G_t \\right] \\\\\n",
    "&= \\mathop{\\mathbb{E}} \\left[ \\sum_t^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t| S_t) G_t \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can update out policy parameters assuming that our policy distribution is differentiable. (This is where some neural networks can be handy)\n",
    "\n",
    "What is actually used in most cases is proximal policy optimisation, which can make this all a bit more stable. See here for more information: https://spinningup.openai.com/en/latest/algorithms/ppo.html \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial fully connected network\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, observation_size, action_size, hidden_size=32):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc_1 = torch.nn.Linear(observation_size, hidden_size)\n",
    "        self.fc_2 = torch.nn.Linear(hidden_size, action_size)\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, observation):\n",
    "        output = self.relu(self.fc_1(observation))\n",
    "        output = self.fc_2(output)\n",
    "        return self.logsoftmax(output)\n",
    "    \n",
    "class PolicyOptimiser:\n",
    "\n",
    "    def __init__(self, observation_size, action_size, hidden_size, learning_rate=0.001, gamma=0.99):\n",
    "        \n",
    "        # setup network and optimiser\n",
    "        self.policy = PolicyNetwork(observation_size, action_size, hidden_size)\n",
    "        self.optimiser = torch.optim.AdamW(self.policy.parameters(), lr=learning_rate)\n",
    "        # define the discount factor\n",
    "        self.gamma = gamma\n",
    "        # resets the training data stored in this class\n",
    "        self.reset()\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        \"\"\"choose actions to take based on the log probabilities from the policy network\n",
    "        \"\"\"\n",
    "        # get log probabilities\n",
    "        action_log_probabilities = self.policy(observation)\n",
    "        # create categorical distribution with these probabilities\n",
    "        action_distribution = torch.distributions.categorical.Categorical(logits=action_log_probabilities)\n",
    "        # sample from this distribution and return log probs\n",
    "        return action_distribution.sample(), action_log_probabilities\n",
    "    \n",
    "    def update_action_rewards(self, observation, action, reward):\n",
    "        self.actions.append(action)\n",
    "        self.observations.append(observation)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update_policy(self,):\n",
    "\n",
    "        # converting things to torch Tensors\n",
    "        returns = self.compute_returns(self.rewards)\n",
    "\n",
    "        actions = torch.from_numpy(np.array(self.actions))\n",
    "        rewards = torch.from_numpy(np.array(self.rewards)).to(torch.float32)\n",
    "        returns = torch.from_numpy(np.array(returns)).to(torch.float32)\n",
    "        observations = torch.from_numpy(np.array(self.observations)).to(torch.float32)\n",
    "\n",
    "\n",
    "        # zero all gradients\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        # compute log probabilities of the observations \n",
    "        action_log_probabilities = self.policy(observations)\n",
    "\n",
    "        # normalise rewards \n",
    "        returns = (returns - returns.mean()) / (returns.std())\n",
    "    \n",
    "        # compute probs for the actions taken, weight by rewards and take mean\n",
    "        loss = -(action_log_probabilities.gather(1, actions.view(-1,1)) * returns).mean()\n",
    "\n",
    "        # update network\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.observations = []\n",
    "        self.masks = []\n",
    "\n",
    "    \n",
    "    def compute_returns(self,rewards, masks=False):\n",
    "        R = 0\n",
    "        returns = []\n",
    "        # loop over the rewards and compute the returns\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            if masks:\n",
    "                R = rewards[step] + self.gamma * R * masks[step]\n",
    "            else:\n",
    "                R = rewards[step] + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_po = BikeRide((6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt = PolicyOptimiser(np.prod(env_po.observation_shape), env.action_space.n, 128, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise some variabls like number of eposides and retrns/losses\n",
    "num_episodes = 15000\n",
    "printdiv = num_episodes//20\n",
    "popt_update_frequency = 20\n",
    "po_returns = []\n",
    "po_losses = []\n",
    "po_game_lengths = []\n",
    "# main training loop\n",
    "for episode in range(num_episodes):\n",
    "    # reset the environment every episode\n",
    "    state, _ = env_po.reset()\n",
    "    # also reset the training data saved to the policy trainer\n",
    "    done = False\n",
    "    it = 0\n",
    "    while not done:\n",
    "        # select an action from the policy network\n",
    "        action, action_log_prob = popt.select_action(torch.from_numpy(state).flatten().unsqueeze(0).to(torch.float32))\n",
    "        # take a step based on the action choice\n",
    "        new_state, reward, done, _, [ ] = env_po.step(action[0].numpy())\n",
    "        # update the training data for the policy\n",
    "        popt.update_action_rewards(new_state.flatten(), action, reward)\n",
    "        # dont let the number of actions in one game exceed 1000\n",
    "        if it > 1000:\n",
    "            done = True\n",
    "        it += 1\n",
    "        \n",
    "    if episode % 1 == 0:\n",
    "        po_game_lengths.append(it)\n",
    "        po_returns.append(env_po.ep_return)\n",
    "        po_losses.append(loss)\n",
    "\n",
    "    if episode % popt_update_frequency == 0:\n",
    "        loss = popt.update_policy()\n",
    "        popt.reset()\n",
    "\n",
    "    if episode % printdiv == 0:\n",
    "        print(f\"Episode: {episode}, reward: {reward}, return: {env_po.ep_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(po_returns, po_losses, po_game_lengths, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_all_positions, po_rewards, po_enjoyments, po_ep_return  = play_game(env_po, agent=popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ani = plot_game_elements(env_po,po_all_positions,po_enjoyments)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic methods\n",
    "\n",
    "Try and implement an actor critic method based on the above two approaches.\n",
    "\n",
    "Here there will be two neural networks, the Actor which predicts the probabilities of taking the next action and the Critic which will predict the Value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
